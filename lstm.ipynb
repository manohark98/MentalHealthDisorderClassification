{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5LADJHmg3uL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "feebc948-38e4-4d07-efff-cbcd6129306c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchbnn\n",
            "  Downloading torchbnn-1.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading torchbnn-1.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: torchbnn\n",
            "Successfully installed torchbnn-1.2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "%pip install torchbnn\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "data = pd.read_csv('CombinedData.csv')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "glove_path = '/content/drive/MyDrive/glove.6B.300d.txt'\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m87GfIZS8lio",
        "outputId": "f704bdab-852c-4e7d-95f0-919ac6daedaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import matplotlib.pyplot as plt\n",
        "nltk.download('punkt')\n",
        "ps = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "import string\n",
        "exclude = string.punctuation\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def clean_text(text):\n",
        "    # Lowering letters\n",
        "    text = text.lower()\n",
        "    # Removing html tags\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    # Removing emails\n",
        "    text = re.sub('\\S*@\\S*\\s?', '', text)\n",
        "    # Removing urls\n",
        "    text = re.sub('https?://[A-Za-z0-9]','',text)\n",
        "    # Removing numbers\n",
        "    text = re.sub('[^a-zA-Z]',' ',text)\n",
        "    word_tokens = word_tokenize(text)\n",
        "    filtered_sentence = []\n",
        "    for word_token in word_tokens:\n",
        "        if word_token not in stop_words:\n",
        "            filtered_sentence.append(word_token)\n",
        "\n",
        "    # Joining words\n",
        "    text = (' '.join(filtered_sentence))\n",
        "    #print(len(text))\n",
        "    return text\n",
        "\n",
        "\n",
        "data_cleaned = data[['statement', 'status']].dropna()\n",
        "\n",
        "data_cleaned['statement'] = data_cleaned['statement'].apply(clean_text)\n",
        "data_cleaned = data_cleaned[['statement', 'status']].dropna(subset=['statement'])\n",
        "\n",
        "\n",
        "\n",
        "#data_cleaned['statement'] = data_cleaned['statement'].apply(lambda x: remove_stopwords(x))"
      ],
      "metadata": {
        "id": "DyGn6x3RhR3Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55103eaf-a817-4d45-8b68-14c937a6d56f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "data = data_cleaned[['statement', 'status']]\n",
        "# Load GloVe embeddings from file\n",
        "def load_glove_embeddings(glove_file_path, embedding_dim=300):\n",
        "    embeddings = {}\n",
        "    with open(glove_file_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = torch.tensor([float(val) for val in values[1:]], dtype=torch.float)\n",
        "            embeddings[word] = vector\n",
        "    return embeddings\n",
        "\n",
        "# Replace with the path to your GloVe file\n",
        "\n",
        "glove_embeddings = load_glove_embeddings(glove_path)\n",
        "\n",
        "# Tokenizer function\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Encode each sentence with GloVe embeddings\n",
        "def encode_sentence(sentence):\n",
        "    tokens = tokenize(sentence)\n",
        "    embeddings = [glove_embeddings[token] for token in tokens if token in glove_embeddings]\n",
        "    return torch.stack(embeddings) if embeddings else torch.zeros((1, 300))\n",
        "\n",
        "data['embeddings'] = data['statement'].apply(encode_sentence)\n",
        "# Preprocess the data\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "data['status'] = label_encoder.fit_transform(data['status'])\n",
        "\n",
        "\n",
        "# Split into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "train_data.reset_index(drop=True, inplace=True)\n",
        "test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Custom Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.embeddings = data['embeddings']\n",
        "        self.labels = data['status']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "# Collate function to pad sequences\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts_padded = pad_sequence(texts, batch_first=True)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return texts_padded, labels\n",
        "\n",
        "train_dataset = TextDataset(train_data)\n",
        "test_dataset = TextDataset(test_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1XGOhx84hXS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "\n",
        "# Define LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers=1, bidirectional=True, dropout=0.5):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        if self.lstm.bidirectional:\n",
        "            hidden = torch.cat((hidden[-2], hidden[-1]), dim=1)\n",
        "        else:\n",
        "            hidden = hidden[-1]\n",
        "        hidden = self.dropout(hidden)\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "# Model parameters\n",
        "embedding_dim = 300\n",
        "hidden_dim = 128\n",
        "output_dim = len(label_encoder.classes_)\n",
        "model = LSTMClassifier(embedding_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for texts, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in test_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "    accuracy = total_correct / total_samples\n",
        "    return accuracy\n",
        "\n",
        "# Function to test the model on a sample input\n",
        "def test_sample(model, text):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embedding = encode_sentence(text).unsqueeze(0).to(device)\n",
        "        output = model(embedding)\n",
        "        prediction = torch.argmax(output, dim=1).item()\n",
        "    return label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer,num_epochs)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate on test data\n",
        "    test_accuracy = evaluate_model(model, test_loader)\n",
        "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "xzY4n29bhZ5X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086f9f73-ab0a-4670-e966-9e1b0ad10ef8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 1317/1317 [00:19<00:00, 68.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Loss: 0.9431\n",
            "Test Accuracy: 72.10%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 1317/1317 [00:19<00:00, 68.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/30], Loss: 0.7196\n",
            "Test Accuracy: 74.93%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 1317/1317 [00:19<00:00, 68.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/30], Loss: 0.6420\n",
            "Test Accuracy: 75.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 1317/1317 [00:19<00:00, 66.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/30], Loss: 0.5871\n",
            "Test Accuracy: 75.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 1317/1317 [00:19<00:00, 67.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/30], Loss: 0.5381\n",
            "Test Accuracy: 76.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 1317/1317 [00:19<00:00, 67.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/30], Loss: 0.4942\n",
            "Test Accuracy: 76.23%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 1317/1317 [00:19<00:00, 66.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/30], Loss: 0.4498\n",
            "Test Accuracy: 75.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 1317/1317 [00:19<00:00, 66.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/30], Loss: 0.4111\n",
            "Test Accuracy: 76.73%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 1317/1317 [00:19<00:00, 67.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/30], Loss: 0.3737\n",
            "Test Accuracy: 76.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 1317/1317 [00:19<00:00, 68.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/30], Loss: 0.3413\n",
            "Test Accuracy: 76.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 1317/1317 [00:19<00:00, 66.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/30], Loss: 0.3127\n",
            "Test Accuracy: 76.22%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 1317/1317 [00:19<00:00, 68.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/30], Loss: 0.2870\n",
            "Test Accuracy: 76.18%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 1317/1317 [00:19<00:00, 67.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/30], Loss: 0.2601\n",
            "Test Accuracy: 76.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 1317/1317 [00:19<00:00, 67.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/30], Loss: 0.2572\n",
            "Test Accuracy: 75.88%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 1317/1317 [00:19<00:00, 67.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/30], Loss: 0.2305\n",
            "Test Accuracy: 75.78%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 1317/1317 [00:19<00:00, 67.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/30], Loss: 0.2049\n",
            "Test Accuracy: 75.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17: 100%|██████████| 1317/1317 [00:19<00:00, 68.01it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/30], Loss: 0.1935\n",
            "Test Accuracy: 75.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18: 100%|██████████| 1317/1317 [00:19<00:00, 66.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/30], Loss: 0.1766\n",
            "Test Accuracy: 75.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 19: 100%|██████████| 1317/1317 [00:19<00:00, 68.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/30], Loss: 0.1696\n",
            "Test Accuracy: 75.34%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 20: 100%|██████████| 1317/1317 [00:19<00:00, 67.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/30], Loss: 0.1535\n",
            "Test Accuracy: 75.47%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 21: 100%|██████████| 1317/1317 [00:19<00:00, 67.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/30], Loss: 0.1508\n",
            "Test Accuracy: 75.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 22: 100%|██████████| 1317/1317 [00:19<00:00, 67.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/30], Loss: 0.1319\n",
            "Test Accuracy: 75.40%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 23: 100%|██████████| 1317/1317 [00:19<00:00, 67.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/30], Loss: 0.1180\n",
            "Test Accuracy: 75.09%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 24: 100%|██████████| 1317/1317 [00:19<00:00, 68.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/30], Loss: 0.1203\n",
            "Test Accuracy: 75.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 25: 100%|██████████| 1317/1317 [00:19<00:00, 67.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/30], Loss: 0.1157\n",
            "Test Accuracy: 75.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 26: 100%|██████████| 1317/1317 [00:19<00:00, 68.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/30], Loss: 0.1045\n",
            "Test Accuracy: 75.21%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 27: 100%|██████████| 1317/1317 [00:19<00:00, 67.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/30], Loss: 0.1136\n",
            "Test Accuracy: 75.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 28: 100%|██████████| 1317/1317 [00:19<00:00, 68.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/30], Loss: 0.0996\n",
            "Test Accuracy: 74.65%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 29: 100%|██████████| 1317/1317 [00:19<00:00, 66.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/30], Loss: 0.0951\n",
            "Test Accuracy: 74.95%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 30: 100%|██████████| 1317/1317 [00:19<00:00, 67.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/30], Loss: 0.0936\n",
            "Test Accuracy: 74.88%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assuming 'model' is your trained LSTM model\n",
        "torch.save(model.state_dict(), 'lstm_model.pth')\n"
      ],
      "metadata": {
        "id": "FqfwMdlmMGxq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from tqdm import tqdm\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Tokenizer function\n",
        "def tokenize(text):\n",
        "    return text.lower().split()\n",
        "\n",
        "# Prepare tokenized sentences for Word2Vec training\n",
        "tokenized_statements = [tokenize(sentence) for sentence in data['statement']]\n",
        "\n",
        "# Train Word2Vec model\n",
        "embedding_dim = 200  # Set your preferred dimension size\n",
        "word2vec_model = Word2Vec(sentences=tokenized_statements, vector_size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Generate a dictionary of Word2Vec embeddings\n",
        "word2vec_embeddings = {word: torch.tensor(word2vec_model.wv[word], dtype=torch.float) for word in word2vec_model.wv.index_to_key}\n",
        "\n",
        "# Encode each sentence using Word2Vec embeddings\n",
        "def encode_sentence(sentence):\n",
        "    tokens = tokenize(sentence)\n",
        "    embeddings = [word2vec_embeddings[token] for token in tokens if token in word2vec_embeddings]\n",
        "    return torch.stack(embeddings) if embeddings else torch.zeros((1, embedding_dim))\n",
        "\n",
        "# Preprocess the data\n",
        "data = data_cleaned[['statement', 'status']]\n",
        "label_encoder = LabelEncoder()\n",
        "data['status'] = label_encoder.fit_transform(data['status'])\n",
        "\n",
        "# Apply encoding to each statement\n",
        "data['embeddings'] = data['statement'].apply(encode_sentence)\n",
        "\n",
        "# Split into train and test sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "train_data.reset_index(drop=True, inplace=True)\n",
        "test_data.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Custom Dataset and DataLoader setup\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.embeddings = data['embeddings']\n",
        "        self.labels = data['status']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.embeddings[idx], self.labels[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    texts_padded = pad_sequence(texts, batch_first=True)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return texts_padded, labels\n",
        "\n",
        "train_dataset = TextDataset(train_data)\n",
        "test_dataset = TextDataset(test_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BbMcK0xOMjrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define LSTM model\n",
        "class LSTMClassifier(nn.Module):\n",
        "    def __init__(self, embedding_dim, hidden_dim, output_dim, n_layers=1, bidirectional=True, dropout=0.5):\n",
        "        super(LSTMClassifier, self).__init__()\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, (hidden, _) = self.lstm(x)\n",
        "        hidden = torch.cat((hidden[-2], hidden[-1]), dim=1) if self.lstm.bidirectional else hidden[-1]\n",
        "        hidden = self.dropout(hidden)\n",
        "        output = self.fc(hidden)\n",
        "        return output\n",
        "\n",
        "# Model parameters\n",
        "hidden_dim = 128\n",
        "output_dim = len(label_encoder.classes_)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LSTMClassifier(embedding_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "# Function to train the model\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
        "    model.train()\n",
        "\n",
        "    total_loss = 0\n",
        "    for texts, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        texts, labels = texts.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(texts)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in test_loader:\n",
        "            texts, labels = texts.to(device), labels.to(device)\n",
        "            outputs = model(texts)\n",
        "            predictions = torch.argmax(outputs, dim=1)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "    accuracy = total_correct / total_samples\n",
        "    return accuracy\n",
        "\n",
        "# Function to test the model on a sample input\n",
        "def test_sample(model, text):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embedding = encode_sentence(text).unsqueeze(0).to(device)\n",
        "        output = model(embedding)\n",
        "        prediction = torch.argmax(output, dim=1).item()\n",
        "    return label_encoder.inverse_transform([prediction])[0]\n",
        "\n",
        "\n",
        "\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = train_model(model, train_loader, criterion, optimizer,num_epochs)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {train_loss:.4f}\")\n",
        "\n",
        "    # Evaluate on test data\n",
        "    test_accuracy = evaluate_model(model, test_loader)\n",
        "    print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGGZcQOaMvvb",
        "outputId": "20d4ed57-9700-45bc-b1ed-9677c64e077d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1: 100%|██████████| 1317/1317 [00:16<00:00, 78.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/30], Loss: 0.9237\n",
            "Test Accuracy: 71.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 2: 100%|██████████| 1317/1317 [00:17<00:00, 73.47it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/30], Loss: 0.7312\n",
            "Test Accuracy: 73.53%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 3: 100%|██████████| 1317/1317 [00:16<00:00, 80.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/30], Loss: 0.6678\n",
            "Test Accuracy: 75.11%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 4: 100%|██████████| 1317/1317 [00:17<00:00, 77.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/30], Loss: 0.6285\n",
            "Test Accuracy: 75.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 5: 100%|██████████| 1317/1317 [00:16<00:00, 80.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/30], Loss: 0.5943\n",
            "Test Accuracy: 75.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 6: 100%|██████████| 1317/1317 [00:16<00:00, 80.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/30], Loss: 0.5634\n",
            "Test Accuracy: 76.33%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 7: 100%|██████████| 1317/1317 [00:16<00:00, 79.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/30], Loss: 0.5371\n",
            "Test Accuracy: 76.12%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 8: 100%|██████████| 1317/1317 [00:16<00:00, 79.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/30], Loss: 0.5098\n",
            "Test Accuracy: 76.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 9: 100%|██████████| 1317/1317 [00:16<00:00, 80.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/30], Loss: 0.4946\n",
            "Test Accuracy: 76.46%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 10: 100%|██████████| 1317/1317 [00:16<00:00, 80.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/30], Loss: 0.4636\n",
            "Test Accuracy: 76.14%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 11: 100%|██████████| 1317/1317 [00:16<00:00, 80.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [11/30], Loss: 0.4429\n",
            "Test Accuracy: 76.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 12: 100%|██████████| 1317/1317 [00:16<00:00, 79.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [12/30], Loss: 0.4228\n",
            "Test Accuracy: 76.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 13: 100%|██████████| 1317/1317 [00:16<00:00, 81.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [13/30], Loss: 0.4006\n",
            "Test Accuracy: 76.55%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 14: 100%|██████████| 1317/1317 [00:16<00:00, 80.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [14/30], Loss: 0.3841\n",
            "Test Accuracy: 76.20%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 15: 100%|██████████| 1317/1317 [00:16<00:00, 80.28it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [15/30], Loss: 0.3622\n",
            "Test Accuracy: 76.44%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 16: 100%|██████████| 1317/1317 [00:16<00:00, 80.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [16/30], Loss: 0.3535\n",
            "Test Accuracy: 76.02%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 17: 100%|██████████| 1317/1317 [00:16<00:00, 80.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [17/30], Loss: 0.3373\n",
            "Test Accuracy: 76.36%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 18: 100%|██████████| 1317/1317 [00:16<00:00, 79.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [18/30], Loss: 0.3307\n",
            "Test Accuracy: 75.56%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 19: 100%|██████████| 1317/1317 [00:16<00:00, 81.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [19/30], Loss: 0.3176\n",
            "Test Accuracy: 76.05%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 20: 100%|██████████| 1317/1317 [00:16<00:00, 80.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [20/30], Loss: 0.3004\n",
            "Test Accuracy: 75.96%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 21: 100%|██████████| 1317/1317 [00:16<00:00, 80.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [21/30], Loss: 0.2974\n",
            "Test Accuracy: 76.31%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 22: 100%|██████████| 1317/1317 [00:16<00:00, 80.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [22/30], Loss: 0.2815\n",
            "Test Accuracy: 76.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 23: 100%|██████████| 1317/1317 [00:16<00:00, 80.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [23/30], Loss: 0.2718\n",
            "Test Accuracy: 76.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 24: 100%|██████████| 1317/1317 [00:16<00:00, 80.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [24/30], Loss: 0.2694\n",
            "Test Accuracy: 75.79%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 25: 100%|██████████| 1317/1317 [00:16<00:00, 81.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [25/30], Loss: 0.2678\n",
            "Test Accuracy: 75.58%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 26: 100%|██████████| 1317/1317 [00:16<00:00, 80.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [26/30], Loss: 0.2539\n",
            "Test Accuracy: 75.99%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 27: 100%|██████████| 1317/1317 [00:16<00:00, 80.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [27/30], Loss: 0.2459\n",
            "Test Accuracy: 75.57%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 28: 100%|██████████| 1317/1317 [00:16<00:00, 80.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [28/30], Loss: 0.2332\n",
            "Test Accuracy: 75.75%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 29: 100%|██████████| 1317/1317 [00:16<00:00, 80.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [29/30], Loss: 0.2384\n",
            "Test Accuracy: 75.63%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 30: 100%|██████████| 1317/1317 [00:16<00:00, 81.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [30/30], Loss: 0.2236\n",
            "Test Accuracy: 75.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample test\n",
        "sample_text = \"Right now I feel passively suicidal. I do not feel like trying to kill myself, but I strongly hope I do not wake up the next day. I wish I was dead, but I do not have any plans in mind. I absolutely hate myself. I hate my life. I hate that I cannot see myself getting any better. I fucking hate it all. I have been on abilify for a month\"\n",
        "predicted_class = test_sample(model, sample_text)\n",
        "print(f\"Predicted Class for sample text: {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iz-2zk2PMyAB",
        "outputId": "827f3028-7f7c-41fb-a6e4-5a4749d44a21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class for sample text: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to test the model on a sample input\n",
        "def test_sample(model, text):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        embedding = encode_sentence(text).unsqueeze(0).to(device)\n",
        "        output = model(embedding)\n",
        "        prediction = torch.argmax(output, dim=1).item()\n",
        "    predicted_label = label_encoder.inverse_transform([prediction])[0]\n",
        "    return predicted_label\n",
        "\n",
        "# Sample test\n",
        "sample_text = \"I just want to go. Some days feel like I am making progress but then I see something that makes me remember the kind of pain I inflected in the person I love the most in this world, and I just want to fucking die. I did not do these things on purpose, but they happened and I am not being given a second chance to make them right.My tarotist (yeah, yeah, I know) told me that I would be doing alright by January next year but I do not see it coming. I do not see my 22 years on this Earth. I had the best birthday by her side last year and knowing she will not be in this one, I want to peel off my skin, get infected by the environment and drop dead.Yesterday was the last drop, she is hanging out with her ex and some other friends of hers. Her ex does not live near so I am assuming they are staying in her house. I know the girl I love is having a hard time, due to having to hide from her family about her sexual orientation while she is already struggling with trying to become independent and getting out of her toxic household environment. it is hard for her but I wish she would seek out some comfort in me too, but in our situation it is understandable why she does not.She sent a final letter through the app we met telling that she could not be friends with me and to not look out for her, but I am weak as hell. I want to know about her life and see that she is doing well.And yeah, she also played her role into not making the relationship work out but, while I am trying to be logical about it, she is dealing with a lot right now. But I felt so bad in a given situation that, instead of helping out and making her feel at ease, I put my problems on top of hers and that cracked her up. She feels that she cannot trust me, I wished she would see the progresses I have made so far but I feel like it does not even matter anymore. The promises I made her, I could not live up to them when the moment most needed it.I was dealing with a lot of unresolved issues when I went to her hometown, and I feel like shit because I made promises with the extroverted version of myself in mind, but that 'me' had been sleeping/dead for over a year (this pandemic sucked out all my social skills, fucking hell). I could not project myself in the way I knew I had in myself some time ago, I was not ready.I am trying to do better but I want to be seen by her. I never liked the idea of living just for myself, filling myself up with projects that mean little to nothing by the time I go to bed, it is exhausting.We talked about getting married, her coming to my college graduation, moving in with each other, we even made bets in relation to our future kids together. I feel empty as fuck and I cannot take it anymore. she is so beautiful, virtues and flaws, and I was so fucking stupid of making her feel like she should change things about herself when she is already perfect in my eyes, well, perfectly imperfect really. I always set high expectations for myself (me never feeling enough and some other thing related to my family relationships) and I put those same expectations on her, without realizing that those made me feel awful as well.do not go on and tell me \"\"there are plenty of other fish in the sea\"\". I do not fucking care about being romantic with anyone else, it makes me sick to even think about it.When she talks even concrete walls fall in love with her, she is that kind of person, and I took her for granted.I do not think I am going to make it to the next year. While I am not alone and I have a better relationship with my parents and some others, they will never be enough. I am in my hometown right now, staying at my dad's. I even told my psychologist about my suicide plan like a month ago. I know they did everything in their power to help me get out of this mental state but that is it, while I do appreciate them for what they tried to do. But now I just want to die in an accident because I cannot do it on my own and maybe it is better, my family will not know that I intentionally put myself through that anyway. At least I will be a donnor and somebody else will get a chance to live a life they want.I feel so fucking stupid and angry at myself.there are a lot of other things missing here but the post is long enough already. If you made it this far, thank you, you are kind, or just bored, heck if I know.Good luck and take care of your loved ones, do not ever take them for granted, do not be like me. I want to die and stop breathing life into my body\"\n",
        "predicted_class = test_sample(model, sample_text)\n",
        "print(f\"Predicted Class for sample text: {predicted_class}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IxTQUHtpRD9h",
        "outputId": "0b59de6e-b2a6-466e-f99f-576fb3facc5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class for sample text: 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_encoder.classes_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlxb4nwmRgkx",
        "outputId": "d7e59374-639d-4177-d8da-463305113d7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2, 3, 4, 5, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}