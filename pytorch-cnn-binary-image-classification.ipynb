{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yAOxjeAjbV4",
        "outputId": "719959e7-ba17-4eda-a063-7bbc952af43c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\kanda\\AppData\\Local\\Temp\\ipykernel_4092\\1734518665.py:1: DeprecationWarning: \n",
            "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
            "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
            "but was not found to be installed on your system.\n",
            "If this would cause problems for you,\n",
            "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
            "        \n",
            "  import pandas as pd\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kanda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\kanda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\kanda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "data = pd.read_csv('CombinedData.csv')\n",
        "# Download NLTK data (for first-time use)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize necessary components\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Data Cleaning Function\n",
        "def clean_text(text):\n",
        "    # Lowering letters\n",
        "    text = text.lower()\n",
        "    # Removing html tags\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    # Removing emails\n",
        "    text = re.sub('\\S*@\\S*\\s?', '', text)\n",
        "    # Removing urls\n",
        "    text = re.sub('https?://[A-Za-z0-9]','',text)\n",
        "    # Removing numbers\n",
        "    text = re.sub('[^a-zA-Z]',' ',text)\n",
        "    word_tokens = word_tokenize(text)    \n",
        "    filtered_sentence = []\n",
        "    for word_token in word_tokens:\n",
        "        if word_token not in stop_words:\n",
        "            filtered_sentence.append(word_token)\n",
        "    \n",
        "    # Joining words\n",
        "    text = (' '.join(filtered_sentence))\n",
        "    #print(len(text))\n",
        "    return text\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming `data` is a pandas DataFrame containing the statements and status columns\n",
        "data_cleaned = data[['statement', 'status']].dropna()\n",
        "\n",
        "# Apply text cleaning to the 'statement' column\n",
        "data_cleaned['cleaned_statement'] = data_cleaned['statement'].apply(clean_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKbk-AMSjA3Z",
        "outputId": "a0925616-ae5d-44a5-ed2f-4d0051291f11"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchbnn in c:\\users\\kanda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Epoch [1/50], Loss: 1.4155\n",
            "Epoch [2/50], Loss: 0.9514\n",
            "Epoch [3/50], Loss: 0.7985\n",
            "Epoch [4/50], Loss: 0.7001\n",
            "Epoch [5/50], Loss: 0.6265\n",
            "Epoch [6/50], Loss: 0.5683\n",
            "Epoch [7/50], Loss: 0.5176\n",
            "Epoch [8/50], Loss: 0.4752\n",
            "Epoch [9/50], Loss: 0.4309\n",
            "Epoch [10/50], Loss: 0.3921\n",
            "Epoch [11/50], Loss: 0.3540\n",
            "Epoch [12/50], Loss: 0.3106\n",
            "Epoch [13/50], Loss: 0.2721\n",
            "Epoch [14/50], Loss: 0.2417\n",
            "Epoch [15/50], Loss: 0.2118\n",
            "Epoch [16/50], Loss: 0.1773\n",
            "Epoch [17/50], Loss: 0.1546\n",
            "Epoch [18/50], Loss: 0.1305\n",
            "Epoch [19/50], Loss: 0.1136\n",
            "Epoch [20/50], Loss: 0.0957\n",
            "Epoch [21/50], Loss: 0.0835\n",
            "Epoch [22/50], Loss: 0.0697\n",
            "Epoch [23/50], Loss: 0.0618\n",
            "Epoch [24/50], Loss: 0.0580\n",
            "Epoch [25/50], Loss: 0.0512\n",
            "Epoch [26/50], Loss: 0.0470\n",
            "Epoch [27/50], Loss: 0.0417\n",
            "Epoch [28/50], Loss: 0.0365\n",
            "Epoch [29/50], Loss: 0.0339\n",
            "Epoch [30/50], Loss: 0.0340\n",
            "Epoch [31/50], Loss: 0.0291\n",
            "Epoch [32/50], Loss: 0.0287\n",
            "Epoch [33/50], Loss: 0.0260\n",
            "Epoch [34/50], Loss: 0.0250\n",
            "Epoch [35/50], Loss: 0.0252\n",
            "Epoch [36/50], Loss: 0.0237\n",
            "Epoch [37/50], Loss: 0.0214\n",
            "Epoch [38/50], Loss: 0.0217\n",
            "Epoch [39/50], Loss: 0.0187\n",
            "Epoch [40/50], Loss: 0.0195\n",
            "Epoch [41/50], Loss: 0.0191\n",
            "Epoch [42/50], Loss: 0.0174\n",
            "Epoch [43/50], Loss: 0.0175\n",
            "Epoch [44/50], Loss: 0.0179\n",
            "Epoch [45/50], Loss: 0.0153\n",
            "Epoch [46/50], Loss: 0.0148\n",
            "Epoch [47/50], Loss: 0.0157\n",
            "Epoch [48/50], Loss: 0.0141\n",
            "Epoch [49/50], Loss: 0.0144\n",
            "Epoch [50/50], Loss: 0.0146\n",
            "Test Accuracy: 75.58%\n"
          ]
        }
      ],
      "source": [
        "%pip install torchbnn\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchbnn as bnn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Data Preprocessing (assuming the data is in 'data_cleaned' DataFrame)\n",
        "# Encode the labels (mental health categories)\n",
        "label_encoder = LabelEncoder()\n",
        "data_cleaned['status_encoded'] = label_encoder.fit_transform(data_cleaned['status'])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data_cleaned['statement'],\n",
        "    data_cleaned['status_encoded'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=data_cleaned['status_encoded']\n",
        ")\n",
        "\n",
        "# Convert text data into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# DataLoader for batching\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define a Bayesian Neural Network (with Dropout as a proxy for uncertainty estimation)\n",
        "class BayesianNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(BayesianNN, self).__init__()\n",
        "        # Bayesian Linear layers with prior distributions\n",
        "        self.blinear1 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=input_size, out_features=hidden_size)\n",
        "        self.blinear2 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=hidden_size, out_features=hidden_size)\n",
        "        self.blinear3 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=hidden_size, out_features=hidden_size)\n",
        "        self.blinear4 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=hidden_size, out_features=output_size)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.blinear1(x))\n",
        "        x = F.relu(self.blinear2(x))\n",
        "        x = F.relu(self.blinear3(x))\n",
        "        x = self.blinear4(x)\n",
        "        return x\n",
        "\n",
        "# Model parameters\n",
        "input_size = 3000  # Number of features (TF-IDF)\n",
        "hidden_size = 256  # Number of neurons in hidden layers\n",
        "output_size = len(label_encoder.classes_)  # Number of categories (mental health statuses)\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "model = BayesianNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=100):\n",
        "    model.train()  # Set model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Testing function\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=50)\n",
        "\n",
        "# Test the model\n",
        "test_model(model, test_loader)\n",
        "\n",
        "# Function to classify new input text\n",
        "def classify_text(model, text, vectorizer, label_encoder):\n",
        "    model.eval()\n",
        "    # Vectorize the input text using the same TF-IDF vectorizer\n",
        "    text_vector = vectorizer.transform([text]).toarray()\n",
        "    text_tensor = torch.tensor(text_vector, dtype=torch.float32)\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(text_tensor)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    predicted_label = label_encoder.inverse_transform([predicted.item()])\n",
        "    return predicted_label[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grWTQXG9jZlW",
        "outputId": "01ac6b58-9083-4d9f-cf9e-adddb0de6c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The predicted mental health category is: Anxiety\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "new_text = \"I am feeling very anxious and stressed about my work.\"\n",
        "predicted_category = classify_text(model, new_text, vectorizer, label_encoder)\n",
        "print(f\"The predicted mental health category is: {predicted_category}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
