{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_yAOxjeAjbV4",
        "outputId": "719959e7-ba17-4eda-a063-7bbc952af43c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:29: SyntaxWarning: invalid escape sequence '\\S'\n",
            "<>:29: SyntaxWarning: invalid escape sequence '\\S'\n",
            "C:\\Users\\kanda\\AppData\\Local\\Temp\\ipykernel_13880\\687301051.py:29: SyntaxWarning: invalid escape sequence '\\S'\n",
            "  text = re.sub('\\S*@\\S*\\s?', '', text)\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\kanda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\kanda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\kanda\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "data = pd.read_csv('CombinedData.csv')\n",
        "# Download NLTK data (for first-time use)\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# Initialize necessary components\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Data Cleaning Function\n",
        "def clean_text(text):\n",
        "    # Lowering letters\n",
        "    text = text.lower()\n",
        "    # Removing html tags\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    # Removing emails\n",
        "    text = re.sub('\\S*@\\S*\\s?', '', text)\n",
        "    # Removing urls\n",
        "    text = re.sub('https?://[A-Za-z0-9]','',text)\n",
        "    # Removing numbers\n",
        "    text = re.sub('[^a-zA-Z]',' ',text)\n",
        "    word_tokens = word_tokenize(text)    \n",
        "    filtered_sentence = []\n",
        "    for word_token in word_tokens:\n",
        "        if word_token not in stop_words:\n",
        "            filtered_sentence.append(word_token)\n",
        "    \n",
        "    # Joining words\n",
        "    text = (' '.join(filtered_sentence))\n",
        "    #print(len(text))\n",
        "    return text\n",
        "\n",
        "# Load your dataset\n",
        "# Assuming `data` is a pandas DataFrame containing the statements and status columns\n",
        "data_cleaned = data[['statement', 'status']].dropna()\n",
        "\n",
        "# Apply text cleaning to the 'statement' column\n",
        "data_cleaned['cleaned_statement'] = data_cleaned['statement'].apply(clean_text)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKbk-AMSjA3Z",
        "outputId": "a0925616-ae5d-44a5-ed2f-4d0051291f11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torchbnn in c:\\users\\kanda\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/100], Loss: 1.4045\n",
            "Epoch [2/100], Loss: 0.8914\n",
            "Epoch [3/100], Loss: 0.7252\n",
            "Epoch [4/100], Loss: 0.6365\n",
            "Epoch [5/100], Loss: 0.5810\n",
            "Epoch [6/100], Loss: 0.5340\n",
            "Epoch [7/100], Loss: 0.4951\n",
            "Epoch [8/100], Loss: 0.4600\n",
            "Epoch [9/100], Loss: 0.4261\n",
            "Epoch [10/100], Loss: 0.3922\n",
            "Epoch [11/100], Loss: 0.3544\n",
            "Epoch [12/100], Loss: 0.3211\n",
            "Epoch [13/100], Loss: 0.2847\n",
            "Epoch [14/100], Loss: 0.2499\n",
            "Epoch [15/100], Loss: 0.2171\n",
            "Epoch [16/100], Loss: 0.1897\n",
            "Epoch [17/100], Loss: 0.1609\n",
            "Epoch [18/100], Loss: 0.1375\n",
            "Epoch [19/100], Loss: 0.1179\n",
            "Epoch [20/100], Loss: 0.1025\n",
            "Epoch [21/100], Loss: 0.0901\n",
            "Epoch [22/100], Loss: 0.0827\n",
            "Epoch [23/100], Loss: 0.0749\n",
            "Epoch [24/100], Loss: 0.0645\n",
            "Epoch [25/100], Loss: 0.0597\n",
            "Epoch [26/100], Loss: 0.0531\n",
            "Epoch [27/100], Loss: 0.0522\n",
            "Epoch [28/100], Loss: 0.0505\n",
            "Epoch [29/100], Loss: 0.0449\n",
            "Epoch [30/100], Loss: 0.0418\n",
            "Epoch [31/100], Loss: 0.0423\n",
            "Epoch [32/100], Loss: 0.0407\n",
            "Epoch [33/100], Loss: 0.0373\n",
            "Epoch [34/100], Loss: 0.0307\n",
            "Epoch [35/100], Loss: 0.0331\n",
            "Epoch [36/100], Loss: 0.0318\n",
            "Epoch [37/100], Loss: 0.0301\n",
            "Epoch [38/100], Loss: 0.0312\n",
            "Epoch [39/100], Loss: 0.0292\n",
            "Epoch [40/100], Loss: 0.0265\n",
            "Epoch [41/100], Loss: 0.0254\n",
            "Epoch [42/100], Loss: 0.0253\n",
            "Epoch [43/100], Loss: 0.0235\n",
            "Epoch [44/100], Loss: 0.0228\n",
            "Epoch [45/100], Loss: 0.0218\n",
            "Epoch [46/100], Loss: 0.0233\n",
            "Epoch [47/100], Loss: 0.0227\n",
            "Epoch [48/100], Loss: 0.0212\n",
            "Epoch [49/100], Loss: 0.0232\n",
            "Epoch [50/100], Loss: 0.0188\n",
            "Epoch [51/100], Loss: 0.0198\n",
            "Epoch [52/100], Loss: 0.0189\n",
            "Epoch [53/100], Loss: 0.0203\n",
            "Epoch [54/100], Loss: 0.0189\n",
            "Epoch [55/100], Loss: 0.0178\n",
            "Epoch [56/100], Loss: 0.0171\n",
            "Epoch [57/100], Loss: 0.0170\n",
            "Epoch [58/100], Loss: 0.0175\n",
            "Epoch [59/100], Loss: 0.0184\n",
            "Epoch [60/100], Loss: 0.0178\n",
            "Epoch [61/100], Loss: 0.0161\n",
            "Epoch [62/100], Loss: 0.0158\n",
            "Epoch [63/100], Loss: 0.0152\n",
            "Epoch [64/100], Loss: 0.0160\n",
            "Epoch [65/100], Loss: 0.0153\n",
            "Epoch [66/100], Loss: 0.0170\n",
            "Epoch [67/100], Loss: 0.0159\n",
            "Epoch [68/100], Loss: 0.0155\n",
            "Epoch [69/100], Loss: 0.0151\n",
            "Epoch [70/100], Loss: 0.0154\n",
            "Epoch [71/100], Loss: 0.0148\n",
            "Epoch [72/100], Loss: 0.0154\n",
            "Epoch [73/100], Loss: 0.0147\n",
            "Epoch [74/100], Loss: 0.0133\n",
            "Epoch [75/100], Loss: 0.0148\n",
            "Epoch [76/100], Loss: 0.0134\n",
            "Epoch [77/100], Loss: 0.0132\n",
            "Epoch [78/100], Loss: 0.0141\n",
            "Epoch [79/100], Loss: 0.0138\n",
            "Epoch [80/100], Loss: 0.0139\n",
            "Epoch [81/100], Loss: 0.0129\n",
            "Epoch [82/100], Loss: 0.0131\n",
            "Epoch [83/100], Loss: 0.0130\n",
            "Epoch [84/100], Loss: 0.0125\n",
            "Epoch [85/100], Loss: 0.0123\n",
            "Epoch [86/100], Loss: 0.0140\n",
            "Epoch [87/100], Loss: 0.0131\n",
            "Epoch [88/100], Loss: 0.0132\n",
            "Epoch [89/100], Loss: 0.0124\n",
            "Epoch [90/100], Loss: 0.0129\n",
            "Epoch [91/100], Loss: 0.0137\n",
            "Epoch [92/100], Loss: 0.0129\n",
            "Epoch [93/100], Loss: 0.0134\n",
            "Epoch [94/100], Loss: 0.0114\n",
            "Epoch [95/100], Loss: 0.0109\n",
            "Epoch [96/100], Loss: 0.0107\n",
            "Epoch [97/100], Loss: 0.0110\n",
            "Epoch [98/100], Loss: 0.0125\n",
            "Epoch [99/100], Loss: 0.0115\n",
            "Epoch [100/100], Loss: 0.0113\n",
            "Test Accuracy: 75.11%\n"
          ]
        }
      ],
      "source": [
        "%pip install torchbnn\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torchbnn as bnn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Data Preprocessing (assuming the data is in 'data_cleaned' DataFrame)\n",
        "# Encode the labels (mental health categories)\n",
        "label_encoder = LabelEncoder()\n",
        "data_cleaned['status_encoded'] = label_encoder.fit_transform(data_cleaned['status'])\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data_cleaned['statement'],\n",
        "    data_cleaned['status_encoded'],\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=data_cleaned['status_encoded']\n",
        ")\n",
        "\n",
        "# Convert text data into TF-IDF vectors\n",
        "vectorizer = TfidfVectorizer(max_features=3000, ngram_range=(1, 2))\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(X_train_tfidf.toarray(), dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test_tfidf.toarray(), dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "# DataLoader for batching\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define a Bayesian Neural Network (with Dropout as a proxy for uncertainty estimation)\n",
        "class BayesianNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(BayesianNN, self).__init__()\n",
        "        # Bayesian Linear layers with prior distributions\n",
        "        self.blinear1 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=input_size, out_features=hidden_size)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(hidden_size)  # Add batch normalization after each Bayesian layer\n",
        "        self.blinear2 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=hidden_size, out_features=hidden_size)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
        "        self.blinear3 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=hidden_size, out_features=hidden_size)\n",
        "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
        "        self.blinear4 = bnn.BayesLinear(prior_mu=0, prior_sigma=0.1, in_features=hidden_size, out_features=output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.batch_norm1(self.blinear1(x)))  # Apply normalization after each layer\n",
        "        x = F.relu(self.batch_norm2(self.blinear2(x)))\n",
        "        #x = F.relu(self.batch_norm3(self.blinear3(x)))\n",
        "        x = self.blinear4(x)  # Output layer does not require normalization\n",
        "        return x\n",
        "\n",
        "# Model parameters\n",
        "input_size = 3000  # Number of features (TF-IDF)\n",
        "hidden_size = 256  # Number of neurons in hidden layers\n",
        "output_size = len(label_encoder.classes_)  # Number of categories (mental health statuses)\n",
        "\n",
        "# Instantiate model, loss, and optimizer\n",
        "model = BayesianNN(input_size, hidden_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, criterion, optimizer, num_epochs=100):\n",
        "    model.train()  # Set model to training mode\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()  # Zero the parameter gradients\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
        "\n",
        "# Testing function\n",
        "def test_model(model, test_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print(f'Test Accuracy: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Train the model\n",
        "train_model(model, train_loader, criterion, optimizer, num_epochs=200)\n",
        "\n",
        "# Test the model\n",
        "test_model(model, test_loader)\n",
        "\n",
        "# Function to classify new input text\n",
        "def classify_text(model, text, vectorizer, label_encoder):\n",
        "    model.eval()\n",
        "    # Vectorize the input text using the same TF-IDF vectorizer\n",
        "    text_vector = vectorizer.transform([text]).toarray()\n",
        "    text_tensor = torch.tensor(text_vector, dtype=torch.float32)\n",
        "\n",
        "    # Get model predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(text_tensor)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "    # Decode the predicted label\n",
        "    predicted_label = label_encoder.inverse_transform([predicted.item()])\n",
        "    return predicted_label[0]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grWTQXG9jZlW",
        "outputId": "01ac6b58-9083-4d9f-cf9e-adddb0de6c21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The predicted mental health category is: Depression\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "new_text = \"I am feeling very anxious and stressed about my work.\"\n",
        "predicted_category = classify_text(model, new_text, vectorizer, label_encoder)\n",
        "print(f\"The predicted mental health category is: {predicted_category}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
